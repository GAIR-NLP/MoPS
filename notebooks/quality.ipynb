{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18b96d70-66f7-4a0e-970e-1331c28a9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tyro\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mops.utils import open_jsonl\n",
    "from mops.constants import client, logger, openai_model\n",
    "from mops.prompts import (\n",
    "    COMPLETENESS_SCORE_PROMPT,\n",
    "    FASCINATION_SCORE_PROMPT,\n",
    "    ORIGINALITY_SCORE_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a9ca125-0009-43b5-a22b-c9ea2a5868d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(\n",
    "    client: OpenAI,\n",
    "    content: str,\n",
    "    model: str = openai_model,\n",
    "    temperature: float = 0.6,\n",
    "):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    assert isinstance(response, str)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba0f123c-509c-41f4-a98d-a67fec71c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_extraction(text: str) -> Tuple[str, str]:\n",
    "    def extract_first_uppercase(text):\n",
    "        match = re.search(r\"\\b((0|[1-9]|[1-9][0-9]|100))\\b\", text)\n",
    "        if match:\n",
    "            return match.group()\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    text_list = text.split(\"\\n\")\n",
    "    text_list = [line for line in text_list if line]\n",
    "    if len(text_list) > 1:\n",
    "        score, explanation = text_list[:2]  # choice and explanation\n",
    "    elif len(text_list) == 1:\n",
    "        score, explanation = text_list[0], \"\"\n",
    "    else:\n",
    "        score, explanation = \"\", \"\"\n",
    "    score = extract_first_uppercase(score)\n",
    "    return score, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "09c309ea-c44f-4e40-b720-46da06a252cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_extraction(text: str) -> Tuple[str, str]:\n",
    "    def extract_first_uppercase(text):\n",
    "        match = re.search(r\"\\b((0|[1-9]|[1-9][0-9]|100))\\b\", text)\n",
    "        if match:\n",
    "            return match.group()\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    text_list = text.split(\"\\n\")\n",
    "    text_list = [line for line in text_list if line]\n",
    "    if len(text_list) > 1:\n",
    "        score, explanation = text_list[:2]  # choice and explanation\n",
    "    elif len(text_list) == 1:\n",
    "        score, explanation = text_list[0], \"\"\n",
    "    else:\n",
    "        score, explanation = \"\", \"\"\n",
    "    score = extract_first_uppercase(score)\n",
    "    return score, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "eddef040-f32e-4db0-9ad5-91b775e3d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(\n",
    "    client: OpenAI,\n",
    "    premise_path: Path,\n",
    "    evaluation_dir: Path,\n",
    "    method: str,\n",
    "    prompt: str,\n",
    "    metric: str,\n",
    "    model: str,\n",
    "):\n",
    "    score_dict_path = evaluation_dir / f\"{metric}_{model}.jsonl\"\n",
    "\n",
    "    all_premise_dicts = open_jsonl(premise_path)\n",
    "    all_method_score_dicts = open_jsonl(score_dict_path, create_if_not_exists=True)\n",
    "\n",
    "    logger.info(f\"Load {method} premises from {premise_path}\")\n",
    "    logger.info(f\"Save {metric} evaluation to {score_dict_path}\")\n",
    "\n",
    "    score_dicts = [\n",
    "        score_dict\n",
    "        for score_dict in all_method_score_dicts\n",
    "        if score_dict[\"method\"] == method\n",
    "    ]\n",
    "\n",
    "    existing_ids = [score_dict[\"id\"] for score_dict in score_dicts]\n",
    "    existing_scores = [score_dict[\"score\"] for score_dict in score_dicts]\n",
    "    \n",
    "    premise_dicts = [\n",
    "        premise_dict\n",
    "        for premise_dict in all_premise_dicts\n",
    "        if premise_dict[\"id\"] not in existing_ids\n",
    "    ]\n",
    "\n",
    "    pbar = tqdm(\n",
    "        premise_dicts,\n",
    "        total=len(all_premise_dicts),\n",
    "        initial=len(all_premise_dicts) - len(premise_dicts),\n",
    "    )\n",
    "    pbar.set_description(f\"Evaluating {metric}: {np.mean(existing_scores):.3f}\")\n",
    "\n",
    "    for premise_dict in pbar:\n",
    "        premise_prompt = prompt.format(premise=premise_dict[\"premise\"])\n",
    "        response = get_response(client, premise_prompt, model, temperature=0.0)\n",
    "\n",
    "        score, explanation = score_extraction(response)\n",
    "\n",
    "        if score == \"\":\n",
    "            logger.warning(\n",
    "                f\"No score detected!, id: {premise_dict['id']}, set to score:`0`\"\n",
    "            )\n",
    "            score = \"0\"\n",
    "\n",
    "        score = int(score)\n",
    "        existing_scores.append(score)\n",
    "        pbar.set_description(f\"Evaluating {metric}: {np.mean(existing_scores):.3f}\")\n",
    "\n",
    "        score_dict = dict(\n",
    "            id=premise_dict[\"id\"],\n",
    "            method=method,\n",
    "            score=score,\n",
    "            explanation=explanation,\n",
    "            premise=premise_dict[\"premise\"],\n",
    "        )\n",
    "\n",
    "        with open(score_dict_path, \"a\") as f:\n",
    "            f.write(json.dumps(score_dict) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e5750-6afe-416a-90f8-67253a5d4caa",
   "metadata": {},
   "source": [
    "### Evaluation: Fascination Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d18ac766-251c-456e-95bb-9fc3c6733b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m06-09 14:46:34\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mEvaluation: fascination_score\u001b[0m\n",
      "\u001b[32m06-09 14:46:34\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mMethod:     mops\u001b[0m\n",
      "\u001b[32m06-09 14:46:34\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mModel:      gpt-4-1106-preview\u001b[0m\n",
      "\u001b[32m06-09 14:46:34\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mLoad mops premises from ../assets/premises/mops/moderate.jsonl\u001b[0m\n",
      "\u001b[32m06-09 14:46:34\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mSave fascination_score evaluation to ../assets/premises/evaluation/fascination_score_gpt-4-1106-preview.jsonl\u001b[0m\n",
      "Evaluating fascination_score: 75.662: 100%|█████████████████████████████████████████████████| 1000/1000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "premise_path: Path = Path(\"../assets/premises/mops/moderate.jsonl\")\n",
    "evaluation_dir: Path = Path(\"../assets/premises/evaluation\")\n",
    "method: str = \"mops\"\n",
    "model:  str = \"gpt-4-1106-preview\"\n",
    "metric: str = \"fascination_score\"\n",
    "\n",
    "logger.info(f\"Evaluation: {metric}\")\n",
    "logger.info(f\"Method:     {method}\")\n",
    "logger.info(f\"Model:      {model}\")\n",
    "score(\n",
    "    client,\n",
    "    premise_path,\n",
    "    evaluation_dir,\n",
    "    method=method,\n",
    "    prompt=FASCINATION_SCORE_PROMPT,\n",
    "    metric=metric,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc036fb-3aa4-4a0e-a0a1-80d625d4690d",
   "metadata": {},
   "source": [
    "### Evaluation: Completeness Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "33345eb5-533d-454d-9d12-81b0b0358624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m06-09 14:46:50\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mEvaluation: completeness_score\u001b[0m\n",
      "\u001b[32m06-09 14:46:50\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mMethod:     mops\u001b[0m\n",
      "\u001b[32m06-09 14:46:50\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mModel:      gpt-4-1106-preview\u001b[0m\n",
      "\u001b[32m06-09 14:46:50\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mLoad mops premises from ../assets/premises/mops/moderate.jsonl\u001b[0m\n",
      "\u001b[32m06-09 14:46:50\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mSave completeness_score evaluation to ../assets/premises/evaluation/completeness_score_gpt-4-1106-preview.jsonl\u001b[0m\n",
      "Evaluating completeness_score: 74.780: 100%|████████████████████████████████████████████████| 1000/1000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "premise_path: Path = Path(\"../assets/premises/mops/moderate.jsonl\")\n",
    "evaluation_dir: Path = Path(\"../assets/premises/evaluation\")\n",
    "method: str = \"mops\"\n",
    "model:  str = \"gpt-4-1106-preview\"\n",
    "metric: str = \"completeness_score\"\n",
    "\n",
    "logger.info(f\"Evaluation: {metric}\")\n",
    "logger.info(f\"Method:     {method}\")\n",
    "logger.info(f\"Model:      {model}\")\n",
    "score(\n",
    "    client,\n",
    "    premise_path,\n",
    "    evaluation_dir,\n",
    "    method=method,\n",
    "    prompt=FASCINATION_SCORE_PROMPT,\n",
    "    metric=metric,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda355ff-b857-4c91-a760-50679772b02d",
   "metadata": {},
   "source": [
    "### Evaluation: Originality Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0ddd0785-6c1e-4986-bdb0-01a19b15fd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m06-09 14:46:59\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mEvaluation: originality_score\u001b[0m\n",
      "\u001b[32m06-09 14:46:59\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mMethod:     mops\u001b[0m\n",
      "\u001b[32m06-09 14:46:59\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mModel:      gpt-4-1106-preview\u001b[0m\n",
      "\u001b[32m06-09 14:46:59\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mLoad mops premises from ../assets/premises/mops/moderate.jsonl\u001b[0m\n",
      "\u001b[32m06-09 14:46:59\u001b[0m \u001b[32mINFO    \u001b[0m \u001b[34mSave originality_score evaluation to ../assets/premises/evaluation/originality_score_gpt-4-1106-preview.jsonl\u001b[0m\n",
      "Evaluating originality_score: 60.013: 100%|█████████████████████████████████████████████████| 1000/1000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "premise_path: Path = Path(\"../assets/premises/mops/moderate.jsonl\")\n",
    "evaluation_dir: Path = Path(\"../assets/premises/evaluation\")\n",
    "method: str = \"mops\"\n",
    "model:  str = \"gpt-4-1106-preview\"\n",
    "metric: str = \"originality_score\"\n",
    "\n",
    "logger.info(f\"Evaluation: {metric}\")\n",
    "logger.info(f\"Method:     {method}\")\n",
    "logger.info(f\"Model:      {model}\")\n",
    "score(\n",
    "    client,\n",
    "    premise_path,\n",
    "    evaluation_dir,\n",
    "    method=method,\n",
    "    prompt=FASCINATION_SCORE_PROMPT,\n",
    "    metric=metric,\n",
    "    model=model,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
